{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-usda in /Users/irisliu/.pyenv/versions/3.6.8/lib/python3.6/site-packages (0.4.6)\n",
      "Requirement already satisfied: requests>=2.16.0 in /Users/irisliu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/requests-2.23.0-py3.6.egg (from python-usda) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/irisliu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/certifi-2020.4.5.1-py3.6.egg (from requests>=2.16.0->python-usda) (2020.4.5.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/irisliu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/chardet-3.0.4-py3.6.egg (from requests>=2.16.0->python-usda) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/irisliu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/idna-2.9-py3.6.egg (from requests>=2.16.0->python-usda) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/irisliu/.pyenv/versions/3.6.8/lib/python3.6/site-packages/urllib3-1.25.9-py3.6.egg (from requests>=2.16.0->python-usda) (1.25.9)\n",
      "\u001b[33mYou are using pip version 18.1, however version 20.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import pprint\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "from requests.adapters import HTTPAdapter\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-usda\n",
    "from usda import UsdaClient\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### USDA Client Key ###\n",
    "# client = UsdaClient('ohrJBZWi8Ggdf00mMWbax5fTBFc4mRm0i8kFGu6D')\n",
    "client = UsdaClient('5jy6h1p6kZf5UX6aBXdQr2frrtfvNZKNZrsyxCei')\n",
    "USDA_API_KEY = '5jy6h1p6kZf5UX6aBXdQr2frrtfvNZKNZrsyxCei'\n",
    "\n",
    "## Foursquare credential\n",
    "CITY = 'Los Angeles'\n",
    "CLIENT_ID = 'WB4STVV0BKOZGPDTSYPOKL1PCMN2ENFIE0UEJQF4PIMJPCGU' #Foursquare ID\n",
    "CLIENT_SECRET = 'SNGR2XVXL4BTDRQBQJHLEYYE2ZXDCAQADNMOILCU34ZCGO4H' #Foursquare Secret\n",
    "# CLIENT_ID = 'EKNXUN23A2RVQRND3DLNTNTBWTYPHP512RTLDVWFHHFPJBHX' \n",
    "# CLIENT_SECRET = 'F0TZIGY5BDBL3M4DT4SA3AEYON2RUXCVAQ3XCCFQHUAQ3RT4' \n",
    "# CLIENT_ID = 'E1WJ2PIB4UUZZLS2BPU5VZDHH5QIDDPIVCL3F0N1R2V1HH3Q' \n",
    "# CLIENT_SECRET = 'W3HFEUATX52S5325HZ501100YOWCZ02UZ5WSLVWRW0ZOGFEN' \n",
    "VERSION = '20180323'\n",
    "# VERSION = '20181201'\n",
    "venue_data_path = './venueData'\n",
    "#CATEGORY_ID = '4bf58dd8d48988d14e941735' #Main Food category\n",
    "# Food Category IDs from foursquares API\n",
    "foodcategory_ids={\"burgers\":\"4bf58dd8d48988d16c941735\",\n",
    "                \"american\":\"4bf58dd8d48988d14e941735\",\n",
    "                \"mexican\":\"4bf58dd8d48988d1c1941735\",\n",
    "                \"fastfood\":\"4bf58dd8d48988d16e941735\",\n",
    "                \"salad\":\"4bf58dd8d48988d1bd941735\",\n",
    "                \"coffee_shops\":\"4bf58dd8d48988d1e0931735\",\n",
    "                \"street_food\":\"53e0feef498e5aac066fd8a9\",\n",
    "                \"seafood_restaurants\":\"4bf58dd8d48988d1ce941735\",\n",
    "                \"piazza\":\"4bf58dd8d48988d1ca941735\",\n",
    "                \"sandwich_place\":\"4bf58dd8d48988d1c5941735\",\n",
    "                \"fried_chicken_joint\":\"4d4ae6fc7a7b7dea34424761\",\n",
    "                \"chinese_restaurant\":\"4bf58dd8d48988d145941735\",\n",
    "                \"korean_restaurant\":\"4bf58dd8d48988d113941735\",\n",
    "                \"bakery\":\"4bf58dd8d48988d16a941735\",\n",
    "                \"italian_restaurant\":\"4bf58dd8d48988d110941735\",\n",
    "                \"taco_places\":\"4bf58dd8d48988d151941735\",\n",
    "                \"sushi_restuarant\":\"4bf58dd8d48988d1d2941735\",\n",
    "                \"brewery\":\"50327c8591d4c4b30a586d5d\",\n",
    "                \"asian_restaurant\":\"4bf58dd8d48988d142941735\",\n",
    "                \"steak_house\":\"4bf58dd8d48988d1cc941735\",\n",
    "                \"bbq_joint\":\"4bf58dd8d48988d1df931735\",\n",
    "                }\n",
    "venuenames_ids={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dailyvalue(nut1,nut, nutrient_name, nutrient,portion):\n",
    "    nut[nutrient_name]=(nutrient.value/portion)*100\n",
    "    if nutrient.name.lower() in nut1:\n",
    "        nut1[nutrient_name]=nut1[nutrient_name]+((nutrient.value/portion)*100)\n",
    "    else:\n",
    "        nut1[nutrient_name]=((nutrient.value/portion)* 100)\n",
    "                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Read category venues and get a list of all restaurants under those categories using Foursquare API.\n",
    "This step will run the foursquare API to open a list of venues. **DO NOT RUN this** if you don't need a new set of venue list becasuse there are limits to the API endpoint access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start scraping category venues...\n"
     ]
    }
   ],
   "source": [
    "print('Start scraping category venues...')\n",
    "for i in foodcategory_ids:\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&v={}&categoryId={}&near={}'.format(\n",
    "        CLIENT_ID, \n",
    "        CLIENT_SECRET,\n",
    "        VERSION, \n",
    "        foodcategory_ids[i], \n",
    "        CITY)\n",
    "    params = dict(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    v='20180323',\n",
    "    categoryId=foodcategory_ids[i],\n",
    "    near=CITY\n",
    "    # ll='40.7243,-74.0018',\n",
    "    # query='coffee',\n",
    "    # limit=1\n",
    "    )\n",
    "    results = requests.get(url=url, params=params).json()\n",
    "    # results = requests.get(url).json()\n",
    "    with open(os.path.join(venue_data_path, 'venuedata_'+i+'.json'), 'w+') as f:\n",
    "        json.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Get all venue id in order to get menus for all the venues found in the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Get all venue id...')\n",
    "all_venues_files = [f for f in glob.glob(os.path.join(venue_data_path, \"venuedata_*.json\"))]\n",
    "all_venues={}\n",
    "\n",
    "for i in all_venues_files:\n",
    "    with open(i,'r') as results:\n",
    "        data=json.load(results)\n",
    "    venue=data['response']['venues']\n",
    "    for i in venue:\n",
    "        name=i['name']\n",
    "        all_venues[name]=i['id']\n",
    "# with open('LA.places.053120.csv', newline='') as csvfile:\n",
    "#     la_csv = csv.reader(csvfile)\n",
    "    # for row in la_csv:\n",
    "        # print(row)\n",
    "#     next(la_csv)\n",
    "#     all_venues = [x[2] for x in la_csv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get menus for all the above venue ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #3. Get menu's for all the above venue id's\n",
    "# print('Getting menus from venue id...')\n",
    "# menu_path = './menuData'\n",
    "# for key, id in enumerate(all_venues):\n",
    "# # for key, id in all_venues.items():\n",
    "#     url = 'https://api.foursquare.com/v2/venues/{}/menu?client_id={}&client_secret={}&v={}'.format(\n",
    "#                                         id, \n",
    "#                                         CLIENT_ID, \n",
    "#                                         CLIENT_SECRET, \n",
    "#                                         VERSION)\n",
    "#     # always retry on 429: quota exceeded\n",
    "#     retry_strategy = Retry(\n",
    "#             total=3,\n",
    "#             status_forcelist=[429, 500, 502, 503, 504],\n",
    "#             method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "#         )\n",
    "#     adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "#     http = requests.Session()\n",
    "#     http.mount(\"https://\", adapter)\n",
    "#     http.mount(\"http://\", adapter)\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     results = response.json()\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         print('Saving menu to csv....')\n",
    "#         with open(os.path.join(menu_path, 'data_'+ ''.join(e for e in str(key) if e.isalnum()) +'.json'), 'w+') as f:\n",
    "#             json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting menus from venue id...\n",
      "https://api.foursquare.com/v2/venues/4c35731c3896e21e4249ed90/menu?client_id=WB4STVV0BKOZGPDTSYPOKL1PCMN2ENFIE0UEJQF4PIMJPCGU&client_secret=SNGR2XVXL4BTDRQBQJHLEYYE2ZXDCAQADNMOILCU34ZCGO4H&v=20180323\n",
      "Saving menu to csv....\n",
      "https://api.foursquare.com/v2/venues/49e3c0c1f964a520c7621fe3/menu?client_id=WB4STVV0BKOZGPDTSYPOKL1PCMN2ENFIE0UEJQF4PIMJPCGU&client_secret=SNGR2XVXL4BTDRQBQJHLEYYE2ZXDCAQADNMOILCU34ZCGO4H&v=20180323\n",
      "Saving menu to csv....\n"
     ]
    }
   ],
   "source": [
    "#3. Get menu's for all the above venue id's\n",
    "print('Getting menus from venue id...')\n",
    "menu_path = './'\n",
    "venue_id_list = {'Taco Bell': '4c35731c3896e21e4249ed90', 'In-N-Out Burger': '49e3c0c1f964a520c7621fe3'}\n",
    "for key, id in venue_id_list.items():\n",
    "# for key, id in all_venues.items():\n",
    "    url = 'https://api.foursquare.com/v2/venues/{}/menu?client_id={}&client_secret={}&v={}'.format(\n",
    "                                        id, \n",
    "                                        CLIENT_ID, \n",
    "                                        CLIENT_SECRET, \n",
    "                                        VERSION)\n",
    "    print(url)\n",
    "    # always retry on 429: quota exceeded\n",
    "    retry_strategy = Retry(\n",
    "            total=3,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            method_whitelist=[\"HEAD\", \"GET\", \"OPTIONS\"]\n",
    "        )\n",
    "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "    http = requests.Session()\n",
    "    http.mount(\"https://\", adapter)\n",
    "    http.mount(\"http://\", adapter)\n",
    "    response = requests.get(url)\n",
    "\n",
    "    results = response.json()\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print('Saving menu to csv....')\n",
    "        with open(os.path.join(menu_path, 'data_'+ ''.join(e for e in str(key) if e.isalnum()) +'.json'), 'w+') as f:\n",
    "            json.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scan each menu item to look for food item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid menu files:  4\n"
     ]
    }
   ],
   "source": [
    "menu_path = './menu_chain'# './Menus' # './menuData'\n",
    "all_menu_files = [f for f in glob.glob(menu_path + \"**/*.json\", recursive=True)]\n",
    "\n",
    "# menu_files = [pd.read_csv(f) for f in glob.glob(menu_path + \"**/*.json\", recursive=True)]\n",
    "# menu_files = [json.load(pd.read_csv(f))['meta']['code'] == 200 for f in all_menu_files]\n",
    "\n",
    "menu_files = []\n",
    "for f in all_menu_files:\n",
    "    with open(f,'r') as results:\n",
    "        data=json.load(results)\n",
    "        if data['meta']['code'] == 200 and data['response']['menu']['menus']['count'] > 0:\n",
    "            menu_files.append(f)\n",
    "print('Valid menu files: ', len(menu_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usda_food_search(item):\n",
    "    url = 'https://api.nal.usda.gov/fdc/v1/foods/search?api_key='+ USDA_API_KEY\n",
    "    # USDA_API_KEY\n",
    "    headers={'Content-type':'application/json', 'Accept':'application/json'}\n",
    "    data = {\n",
    "      \"query\": item,\n",
    "      \"pageSize\": 25,\n",
    "      \"sortBy\": \"dataType.keyword\",\n",
    "      \"sortOrder\": \"asc\"\n",
    "    }\n",
    "\n",
    "    r = requests.post(url=url, json=data, headers=headers)\n",
    "    if r.status_code :\n",
    "        result = r.json()\n",
    "        return result\n",
    "    return None\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Analyze the nutrional content for each using USDA nutrient database API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "drink_corpus = ['wine', 'white', 'rosé', 'drink', 'drinks', 'red', 'cocktail', 'martinis', 'sparkling', \n",
    "                'cocktails', 'beverages', 'bptmargaritas', 'beer', 'sangria', 'tequila', 'anejo', \n",
    "               'mezcal', 'reposado', 'blanco / Plata', 'Aguas Frescas', 'bloodymary&mimosacart', \n",
    "                'seasonalspritz', 'aguafrescas', 'californiachardonnay', 'Italian White', 'French White', \n",
    "                'Cabernet', 'Italian Red', 'California Merlot', 'Pinot Noir', 'Bordeaux Red', 'White Wine', \n",
    "                'Red Wine', 'Rose / Sparkling', 'On Tap', 'Proudly Serving Forster & Burnett','Bubbles','Margaritas','Chardonnay','Vinos','Pinot Grigio','Sauvignon Blanc','Rose','Cócteles','Malbec','Cervezas','Bottled','Draft','Wines By the Glass','Margarita Pitchers','Tequila Shots',\n",
    "                'Draught Beer', 'Old World Reds', 'Bottles & Cans', 'Eclectic Reds - New World', 'Cabernet Sauvignon', 'Bordeaux Blend', 'Prosecco', 'Champagne'\n",
    "                'soda', 'coffee', 'Iced Tea', 'Aquas Frescas', 'Xv Coffee'\n",
    "               ]\n",
    "drink_corpus = pd.read_csv('drinks_corpus.csv', delimiter=',', sep='\\t', header=None)\n",
    "\n",
    "def is_drink_item(item):\n",
    "    item = str(item)\n",
    "    clean_food_string = re.sub('\\W+','', item.lower())\n",
    "    for key in drink_corpus:\n",
    "        clean_key = re.sub('\\W+','', key.lower())\n",
    "        if clean_food_string.find(clean_key) != -1 and clean_food_string == clean_key:\n",
    "#             print('>>>>Filtered...: ', item)\n",
    "            return True\n",
    "    return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_drink_section(section):\n",
    "    if not is_drink_item(menu_sub_section['name']):\n",
    "        filtered_menu_sub_section.append(menu_sub_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_dict = defaultdict(list)\n",
    "menu_files_folder_name = 'nutrient_report_Nov_25' #### Change the name here for the nutrient report from USDA ####\n",
    "def scan_food_item_from_main_section(menu_main_section):\n",
    "    \n",
    "    for menu_sub_section in menu_main_section:\n",
    "        \n",
    "        # filter drink section from the menu\n",
    "        filtered_menu_sub_section = []\n",
    "        if not is_drink_item(menu_sub_section['name']):\n",
    "            filtered_menu_sub_section = menu_sub_section\n",
    "            \n",
    "        # add food items from menu's subsections\n",
    "        if filtered_menu_sub_section != [] or filtered_menu_sub_section != None:\n",
    "            if 'name' in filtered_menu_sub_section:\n",
    "                print('menu section included: ', filtered_menu_sub_section['name'])\n",
    "#                 print(filtered_menu_sub_section)\n",
    "                if filtered_menu_sub_section['entries']['count'] > 1:  \n",
    "                    for food in filtered_menu_sub_section['entries']['items']:\n",
    "                        food_dict['name'].append(food['name'])\n",
    "                        if 'description' in food:\n",
    "                            food_dict['meta'].append(food['description'])\n",
    "\n",
    "                else: \n",
    "                    if filtered_menu_sub_section['entries']['count'] > 0:\n",
    "                        food_dict['name'].append(filtered_menu_sub_section['entries']['items'][0]['name'])\n",
    "                        # add description to meta if any\n",
    "                        if 'description' in filtered_menu_sub_section['entries']['items'][0]:\n",
    "                            food_dict['meta'].append(filtered_menu_sub_section['entries']['items'][0]['description'])\n",
    "            \n",
    "    nut={}\n",
    "    nut1={}\n",
    "    count=0\n",
    "    d={}\n",
    "    main_nut=[\"energy\",\"protein\",\"total lipid (fat)\",\"carbohydrate\",\"fiber\",\"sodium\",\"cholesterol\"]\n",
    "    \n",
    "    for food_name in food_dict['name']:\n",
    "        #To get the nutrient decomposition for each.\n",
    "        nut={\"energy\":0,\"protein\":0,\"total lipid (fat)\":0,\"carbohydrate\":0,\"fiber\":0,\"sodium\":0,\"cholesterol\":0}\n",
    "        try:\n",
    "            food_item =  re.sub('[^A-Za-z]+', ' ', food_name)\n",
    "            report = usda_food_search(food_item)\n",
    "            if report != None:\n",
    "                with open(os.path.join(menu_files_folder_name + '/nutrient_'+ location_name + '_' + food_item.replace(\" \", \"_\") + '.json'), 'w+') as file:\n",
    "                    json.dump(report, file)\n",
    "\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping from .... Menus/data_Tocaya Organica.json\n",
      "menu section included:  Bowls\n",
      "menu section included:  Burritos\n",
      "menu section included:  Tacos\n",
      "menu section included:  Salads\n",
      "menu section included:  Quesadillas\n",
      "menu section included:  Sides / Soups\n",
      "menu section included:  Fresh Drinks\n",
      "menu section included:  Bottled Drinks\n",
      "menu section included:  Dessert\n",
      "menu section included:  Signature Catering Packages\n",
      "menu section included:  Burrito Box\n",
      "menu section included:  Taco Bars\n",
      "menu section included:  Bowls\n",
      "menu section included:  Salads\n",
      "menu section included:  Sides & Extras\n",
      "menu section included:  Sides\n",
      "menu section included:  Extras\n",
      "menu section included:  Standard Protein\n",
      "menu section included:  Premium Protein\n",
      "menu section included:  Taco Shells\n",
      "menu section included:  Single Serve\n",
      "menu section included:  Carafes\n",
      "menu section included:  Iced Tea\n",
      "menu section included:  Dessert\n",
      "scraping from .... Menus/data_The Misfit Restaurant + Bar.json\n",
      "menu section included:  Raw Bar\n",
      "menu section included:  Hand Rolls\n",
      "menu section included:  To Start\n",
      "menu section included:  Salads\n",
      "menu section included:  Sandwiches\n",
      "menu section included:  Entrées\n",
      "menu section included:  Skewers\n",
      "menu section included:  From the Field\n",
      "menu section included:  Finish with Some Gelato\n",
      "menu section included:  Brunch\n",
      "menu section included:  Finish with Some\n",
      "menu section included:  Not Brunch\n",
      "menu section included:  Fully Licensed\n",
      "menu section included:  Monday\n",
      "menu section included:  Tuesday\n",
      "menu section included:  Wednesday\n",
      "menu section included:  Thursday\n",
      "menu section included:  Friday\n",
      "scraping from .... Menus/data_The Ivy.json\n",
      "menu section included:  Lunch & Dinner Starters\n",
      "menu section included:  Pizza\n",
      "menu section included:  Main Course Salads\n",
      "menu section included:  From Our Mesquite Grill\n",
      "menu section included:  Main Courses\n",
      "menu section included:  Pasta\n",
      "menu section included:  Side Orders\n",
      "menu section included:  The Ivy Desserts\n",
      "scraping from .... Menus/data_The Original Pantry.json\n",
      "menu section included:  Salads\n",
      "menu section included:  Steaks & Chops\n",
      "menu section included:  Main\n",
      "menu section included:  Side Orders\n",
      "menu section included:  Cakes & Pies\n",
      "menu section included:  Desserts\n",
      "scraping from .... Menus/data_The Rose Venice.json\n",
      "menu section included:  Greenery\n",
      "menu section included:  Handheld\n",
      "menu section included:  Pizza\n",
      "menu section included:  Pasta\n",
      "menu section included:  Sweets\n",
      "menu section included:  Breakfast Favorites\n",
      "scraping from .... Menus/data_Bob_s Burgers.json\n",
      "menu section included:  Catering\n",
      "menu section included:  Salads and Provisiones\n",
      "menu section included:  Tacos\n",
      "menu section included:  Sizzling Fajitas\n",
      "menu section included:  Salsas\n",
      "menu section included:  Sides\n",
      "menu section included:  Dessert\n",
      "menu section included:  Frozen\n",
      "menu section included:  Aquas Frescas\n",
      "scraping from .... Menus/data_The Henry.json\n",
      "menu section included:  Entrees\n",
      "menu section included:  Sides\n",
      "menu section included:  Kids Breakfast Menu\n",
      "menu section included:  Entrees\n",
      "menu section included:  Sides\n",
      "menu section included:  Dessert\n",
      "menu section included:  House Browns\n",
      "menu section included:  Bourbon\n",
      "menu section included:  Rye\n",
      "menu section included:  Scotch\n",
      "menu section included:  Irish\n",
      "menu section included:  Japanese\n",
      "menu section included:  Bubbles & Rosé\n",
      "menu section included:  Champagne\n",
      "menu section included:  Eclectic Whites\n",
      "menu section included:  Espresso\n",
      "menu section included:  Xv Coffee\n",
      "menu section included:  Smoothies\n",
      "menu section included:  Fresh Juice\n",
      "menu section included:  Breakfast\n",
      "menu section included:  Breakfast Sides\n",
      "menu section included:  Lunch\n",
      "menu section included:  Lunch Sides\n",
      "scraping from .... Menus/data_Blue Plate Taco.json\n",
      "menu section included:  Catering\n",
      "menu section included:  Salads and Provisiones\n",
      "menu section included:  Tacos\n",
      "menu section included:  Sizzling Fajitas\n",
      "menu section included:  Salsas\n",
      "menu section included:  Sides\n",
      "menu section included:  Dessert\n",
      "menu section included:  Frozen\n",
      "menu section included:  Aquas Frescas\n",
      "scraping from .... Menus/data_El Coyote.json\n",
      "menu section included:  Appetizers\n",
      "menu section included:  Soups-Salads\n",
      "menu section included:  Especiales De La Casa\n",
      "menu section included:  Side Orders\n",
      "menu section included:  Desserts\n",
      "scraping from .... Menus/data_Gracias Madre.json\n",
      "menu section included:  Small Bites\n",
      "menu section included:  Starters\n",
      "menu section included:  Entrées\n",
      "menu section included:  Taco Tuesday\n",
      "menu section included:  Starters\n",
      "menu section included:  Entrées\n",
      "menu section included:  Dessert\n",
      "menu section included:  Small Bites\n",
      "menu section included:  Starters\n",
      "menu section included:  Entrées\n",
      "menu section included:  Dessert\n",
      "menu section included:  Dessert\n",
      "menu section included:  Bites\n",
      "menu section included:  Tacos\n",
      "menu section included:  To Sip\n",
      "menu section included:  To Start\n",
      "menu section included:  Vegetables + Salads\n",
      "menu section included:  To Follow\n",
      "menu section included:  Bowls\n",
      "menu section included:  Juice, Coffee and Tea\n",
      "menu section included:  Starters\n",
      "menu section included:  Vegetables + Salads\n",
      "menu section included:  Bowls\n",
      "menu section included:  Entrées\n",
      "menu section included:  Juice, Coffee and Tea\n",
      "menu section included:  Starters\n",
      "menu section included:  Vegetables + Salads\n",
      "menu section included:  Tortas\n",
      "menu section included:  Bowls\n",
      "menu section included:  Mains a La Carta\n",
      "menu section included:  Smoothies\n",
      "menu section included:  Juice, Coffee & Tea\n",
      "scraping from .... Menus/data_Toca Madera.json\n",
      "menu section included:  For the Table\n",
      "menu section included:  Raw Bar\n",
      "menu section included:  Taqueria\n",
      "menu section included:  Starters\n",
      "menu section included:  Salads\n",
      "menu section included:  Entrees\n",
      "menu section included:  Taqueria\n"
     ]
    }
   ],
   "source": [
    "\n",
    "total_nutrient_data={}\n",
    "\n",
    "folder_name = \"Menus\" #### Change the name here for the menu data ####\n",
    "menu_files = glob.glob(os.path.join(folder_name, \"*.json\"))\n",
    "for f in menu_files:\n",
    "    menu_main_section=defaultdict(list)\n",
    "    total_nutrient_d={}\n",
    "    location_name=f.split('/')[-1:][0].split('_')[1].split('.')[0]\n",
    "    \n",
    "    with open(f,'r') as results:\n",
    "        print('scraping from ....', f)\n",
    "        data=json.load(results)\n",
    "        \n",
    "        # add menu main sections to list\n",
    "        if data['response']['menu']['menus']['count'] == 1:\n",
    "            menu_main_section = [data['response']['menu']['menus']['items'][0]['entries']['items']]\n",
    "        else:\n",
    "            menu_main_section = [data['response']['menu']['menus']['items'][i]['entries']['items'] for i in range(len(data['response']['menu']['menus']['items']))]\n",
    "   \n",
    "    \n",
    "  \n",
    "    for main_section in menu_main_section:\n",
    "        # filter drink section from the menu\n",
    "        if not is_drink_item(main_section[0]['name']):\n",
    "            scan_food_item_from_main_section(main_section)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Process the results from USDA Food Database and print CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# menu_files_folder_name = 'nutrient_report_Oct_7_new'\n",
    "# nutrient_reports = glob.glob(os.path.join(menu_files_folder_name, \"*.json\"))\n",
    "\n",
    "# Load all nutrient report\n",
    "nutrient_reports = []\n",
    "nutrient_report_names = ['nutrient_report_Nov_25', 'nutrient_reports_4_chain'] #### Change the name here for the nutrient report from USDA ####\n",
    "for files in nutrient_report_names:\n",
    "    nutrient_reports.extend(glob.glob(os.path.join(files, \"*.json\")))\n",
    "\n",
    "# Setup CSV\n",
    "# menu_analysis=\"./nutrient_analysis_\"+ menu_files_folder_name +\".csv\"\n",
    "menu_analysis=\"./nutrient_analysis_Nov_25.csv\" #### FINAL CSV filename here ####\n",
    "csvfile=open(menu_analysis,'w')\n",
    "menu_analysis_f = csv.writer(csvfile)\n",
    "menu_analysis_f.writerow([\"Restaurant Name\",\"Original Menu Item\", \"Matched food Item\", \"Match Score\",\"Energy (KCAL)\",\"Protein (G)\",\"Total lipid(fat) (G)\",\"Carbohydrate (G)\",\"Fiber (G)\",\"Sodium (MG)\",\"Cholesterol (MG)\"])\n",
    "\n",
    "# Average all nutrient content \n",
    "food_score_threshold = 25\n",
    "\n",
    "for report in nutrient_reports:\n",
    "    energy, protein, total_lipid, carbohydrate, fiber, sodium, cholesterol = [],[],[],[],[],[],[] \n",
    "    with open(str(report)) as input_file:\n",
    "        filename = report.split('/')[-1].split('_')\n",
    "        location_name = filename[1]\n",
    "        food = report.split('/')[-1].split('.json')[0].split(location_name)[-1].replace('_', ' ').lstrip()\n",
    "        data = json.load(input_file)\n",
    "        food_benchmark = data[\"foods\"]\n",
    "        match_food_items = []\n",
    "        match_food_score = 0\n",
    "        for food_item in food_benchmark:\n",
    "            pos_correlated_food_search = False\n",
    "            match_food_items.append(food_item['description'])\n",
    "            match_food_score += food_item['score']\n",
    "            food_nutrients = food_item['foodNutrients']\n",
    "            \n",
    "            # Calculate food value if matched food item is positively correlated to original food item\n",
    "            if food_nutrients and match_food_score > food_score_threshold:\n",
    "                pos_correlated_food_search = True\n",
    "                for food_nutrient in food_nutrients:\n",
    "                    if food_nutrient['nutrientId'] == 1003:\n",
    "                        protein.append(food_nutrient['value'])\n",
    "                    if food_nutrient['nutrientId'] == 1004:\n",
    "                        total_lipid.append(food_nutrient['value'])\n",
    "                    if food_nutrient['nutrientId'] == 1008:\n",
    "                        energy.append(food_nutrient['value'])\n",
    "                    if food_nutrient['nutrientId'] == 1005:\n",
    "                        carbohydrate.append(food_nutrient['value'])\n",
    "                    if food_nutrient['nutrientId'] == 1079:\n",
    "                        fiber.append(food_nutrient['value'])\n",
    "                    if food_nutrient['nutrientId'] == 1093:\n",
    "                        sodium.append(food_nutrient['value'])\n",
    "                    if food_nutrient['nutrientId'] == 1253:\n",
    "                        cholesterol.append(food_nutrient['value'])\n",
    "    \n",
    "    # Calculate the average nutrients\n",
    "    if pos_correlated_food_search:\n",
    "        average_protein = sum(protein) / len(protein) if len(protein) > 0 else 0\n",
    "        average_total_lipid = sum(total_lipid) / len(total_lipid) if len(total_lipid) > 0 else 0\n",
    "        average_energy = sum(energy) / len(energy) if len(energy) > 0 else 0\n",
    "        average_carbohydrate = sum(carbohydrate) / len(carbohydrate) if len(carbohydrate) > 0 else 0\n",
    "        average_fiber = sum(fiber) / len(fiber) if len(fiber) > 0 else 0\n",
    "        average_sodium = sum(sodium) / len(sodium) if len(sodium) > 0 else 0\n",
    "        average_cholesterol = sum(cholesterol) / len(cholesterol) if len(cholesterol) > 0 else 0\n",
    "        \n",
    "        average_match_food_score = match_food_score / len(match_food_items) * 1.0\n",
    "\n",
    "        menu_analysis_f.writerow([location_name,food, match_food_items, average_match_food_score, average_energy,average_protein,average_total_lipid,average_carbohydrate, average_fiber,average_sodium,average_cholesterol])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Display top 50 menu nutrient analysis result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "menu_analytic_report_name = \"./nutrient_analysis_Nov_25.csv\" #### FINAL CSV filename here ####\n",
    "nutrient_pd = pd.read_csv(menu_analytic_report_name) \n",
    "print('This table contains: {} rows x {} cols.'.format(nutrient_pd.shape[0], nutrient_pd.shape[1]))\n",
    "nutrient_pd.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
